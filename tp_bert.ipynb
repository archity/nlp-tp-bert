{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CCsTMRfzfgPK",
        "fPaN19rhfgPN",
        "_sr62iV-hYRA",
        "L2mmpMT6fgPO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Practical\n",
        "\n",
        "Practical work on BERT for the course Natural Language Processing in M2 MoSIG\n",
        "\n",
        "> Author: Archit YADAV\n",
        "\n",
        "The final findings and experimentation details can be found in the last markdown section of this notebook titled \"Report\".\n",
        "\n",
        "Table of Contents\n",
        "\n",
        "1. IMPORT MODULES\n",
        "2. DATA\n",
        "3. TOKENISATION\n",
        "4. MODEL\n",
        "5. TRAINING\n",
        "6. PREDICTIONS\n",
        "7. IMPROVE THE MODEL\n",
        "\n",
        "  REPORT"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Iaxs5T3cfgPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.IMPORT MODULES"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CCsTMRfzfgPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "7hdlQuifgDMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4e-cM3FPUE-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Managing arrays\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "# load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available.\")\n",
        "  device = torch.cuda.current_device()\n",
        "else:\n",
        "  print(\"Will work on CPU.\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk0dCwOmfgPM",
        "outputId": "86ac8441-76bf-4991-ffe4-ab15b44875f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.DATA"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fPaN19rhfgPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Downloading of Data"
      ],
      "metadata": {
        "id": "_sr62iV-hYRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: \n",
            "3885 posts in total\n",
            "\t 2332 training posts\n",
            "\t\t 593 comp.windows.x\n",
            "\t\t 594 sci.med\n",
            "\t\t 599 soc.religion.christian\n",
            "\t\t 546 talk.politics.guns\n",
            "\t 1553 testing posts\n",
            "\t\t 395 comp.windows.x\n",
            "\t\t 396 sci.med\n",
            "\t\t 398 soc.religion.christian\n",
            "\t\t 364 talk.politics.guns\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        " 'comp.windows.x',\n",
        " 'sci.med',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        "]\n",
        "\n",
        "# Download data if not already present in data_home\n",
        "trainset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "testset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "\n",
        "# Define input data and labels for training and testing\n",
        "x_train = trainset.data\n",
        "y_train = trainset.target\n",
        "x_test = testset.data\n",
        "y_test = testset.target\n",
        "\n",
        "# SOLUTION (yes, we are cool)\n",
        "print('Dataset size: \\n{} posts in total'.format(len(x_train) + len(x_test)))\n",
        "print('\\t {} training posts'.format(len(x_train)))\n",
        "\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_train == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "\n",
        "print('\\t {} testing posts'.format(len(x_test)))\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_test == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "\n",
        "\n",
        "\n",
        "# print('\\n')\n",
        "# print('EXAMPLE: \\n')\n",
        "# print(x_train[0])\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaiRgy0yfgPN",
        "outputId": "eebe3e6f-6aeb-4265-b3b5-746352501f50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Cleaning of Data"
      ],
      "metadata": {
        "id": "MRgRZaothcE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove lines starting with certain keywords \n",
        "def clean_post(post: str, remove_start: tuple):\n",
        "    clean_lines = []\n",
        "    for line in post.splitlines():\n",
        "            if not line.startswith(remove_start):\n",
        "                clean_lines.append(line)\n",
        "    return '\\n'.join(clean_lines)\n",
        "    \n",
        "\n",
        "# SOLUTION (yes, again, we are cool)\n",
        "remove_start = (\n",
        "  'From:',\n",
        "  'Subject:',\n",
        "  'Reply-To:',\n",
        "  'In-Reply-To:',\n",
        "  'Nntp-Posting-Host:',\n",
        "  'Organization:',\n",
        "  'X-Mailer:',\n",
        "  'In article <',\n",
        "  'Lines:',\n",
        "  'NNTP-Posting-Host:',\n",
        "  'Summary:',\n",
        "  'Article-I.D.:'\n",
        ")\n",
        "x_train = [clean_post(p, remove_start) for p in x_train]\n",
        "x_test = [clean_post(p, remove_start) for p in x_test]\n"
      ],
      "metadata": {
        "id": "Ma4tJ6NKhTSR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.TOKENISATION"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "L2mmpMT6fgPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n",
        "\n",
        "# Let's check out how the tokenizer works\n",
        "for n in range(3):\n",
        "    # Tokenize forum post\n",
        "    tokenizer_out = tokenizer(x_train[n])\n",
        "    # Convert numerical tokens to alphabetical tokens\n",
        "    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n",
        "    # Decode tokens back to string\n",
        "    decoded = tokenizer.decode(tokenizer_out.input_ids)\n",
        "    print(tokenizer_out)\n",
        "    print(encoded_tok, '\\n')\n",
        "    print(decoded, '\\n')\n",
        "    print('---------------- \\n')\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QaJLo-4wfgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "class PostsDataset(Dataset):\n",
        "    def __init__(self, posts, labels, tokenizer, max_len):\n",
        "        # Variables that are set when the class is instantiated\n",
        "        self.posts = posts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.posts)\n",
        "  \n",
        "    def __getitem__(self, item):\n",
        "        # Select the post and its category\n",
        "        post = str(self.posts[item])\n",
        "        label = self.labels[item]\n",
        "        # Tokenize the post\n",
        "        tokenizer_out = self.tokenizer(\n",
        "            post,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "            )\n",
        "        # Return a dictionary with the output of the tokenizer and the label\n",
        "        return  {\n",
        "            'input_ids': tokenizer_out['input_ids'].flatten(),\n",
        "            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Instantiate two PostsDatasets\n",
        "train_dataset = PostsDataset(x_train, y_train, tokenizer, MAX_LEN)\n",
        "test_dataset = PostsDataset(x_test, y_test, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "Q-TwXpvoksBc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.MODEL"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eY2w_htDfgPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        " \n",
        "first_post = train_dataset[0]\n",
        "\n",
        "hidden_state = distilbert(\n",
        "    input_ids=first_post['input_ids'].unsqueeze(0),\n",
        "    attention_mask=first_post['attention_mask'].unsqueeze(0)\n",
        "    )\n",
        "\n",
        "print(hidden_state[0].shape)\n",
        "print(distilbert.config)\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xnhWAdLNfgPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertPreTrainedModel, DistilBertConfig\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "class DistilBertForPostClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, freeze_encoder=False):\n",
        "        # Instantiate the parent class DistilBertPreTrainedModel\n",
        "        super().__init__(config)\n",
        "        # Instantiate num. of classes\n",
        "        self.num_labels = num_labels\n",
        "        # Instantiate and load a pretrained DistilBERT model as encoder\n",
        "        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        # Freeze the encoder parameters if required (Q1)\n",
        "        if freeze_encoder:\n",
        "          for param in self.encoder.parameters():\n",
        "              param.requires_grad = False\n",
        "        \n",
        "        # SOLUTION 3\n",
        "        # An extra proposed layer\n",
        "        self.ExtraLayer = torch.nn.Linear(\n",
        "            in_features=config.dim, out_features=(config.dim-32), bias=True)\n",
        "\n",
        "        # The classifier: a feed-forward layer attached to the encoder's head\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            in_features=(config.dim-32), out_features=self.num_labels, bias=True)\n",
        "        # Instantiate a dropout function for the classifier's input\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "    ):\n",
        "        # Encode a batch of sequences with DistilBERT\n",
        "        encoder_output = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        # Extract the hidden representations from the encoder output\n",
        "        hidden_state = encoder_output[0]  # (bs, seq_len, dim)\n",
        "        # Only select the encoding corresponding to the first token\n",
        "        # of each sequence in the batch (Q2)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        # Apply dropout\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        \n",
        "        # SOLUITION 3\n",
        "        # Feed into our extra layer\n",
        "        pooled_output = self.ExtraLayer(pooled_output)\n",
        "        # Pass it through the ReLU function\n",
        "        pooled_output = F.relu(pooled_output)\n",
        "\n",
        "        # Feed into the classifier\n",
        "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
        "\n",
        "        outputs = (logits,) + encoder_output[1:]\n",
        "        \n",
        "        if labels is not None: # (Q3)\n",
        "          # Instantiate loss function\n",
        "          # SOLUTION :\n",
        "          loss_fct = torch.nn.CrossEntropyLoss()\n",
        "          # Calculate loss\n",
        "          # SOLUTION :\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "          # Aggregate outputs\n",
        "          outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "model = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = True\n",
        "    )\n",
        "\n",
        "model_unfreezed = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = False)\n",
        "\n",
        "# Print info about model's parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print('Model total params: ', total_params)\n",
        "print('Model trainable params: ', trainable_params)\n",
        "print('\\n', model)"
      ],
      "metadata": {
        "id": "pCuSAhqRnFvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.TRAINING"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b1sqdw_-fgPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    logging_dir='./logs',\n",
        "    logging_first_step=True,\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=10,              \n",
        "    per_device_train_batch_size=8,  \n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01        \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_unfreezed = Trainer(\n",
        "    model=model_unfreezed,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Freeze version\n",
        "# train_results = trainer.train()\n",
        "# test_results = trainer.predict(test_dataset=test_dataset)\n",
        "\n",
        "# Unfreeze version\n",
        "train_results = trainer_unfreezed.train()\n",
        "test_results = trainer_unfreezed.predict(test_dataset=test_dataset)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yzNYv5-nfgPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Predictions: \\n', test_results.predictions)\n",
        "print('\\nAccuracy: ', test_results.metrics['test_accuracy'])\n",
        "print('Precision: ', test_results.metrics['test_precision'])\n",
        "print('Recall: ', test_results.metrics['test_recall'])\n",
        "print('F1: ', test_results.metrics['test_f1'])\n",
        "print(categories)\n",
        "\n",
        "MODEL_PATH = './my_model'\n",
        "trainer.save_model(MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB5679XnXkHH",
        "outputId": "e512ab1d-9c64-4d1a-be6a-7acb379939c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./my_model\n",
            "Configuration saved in ./my_model/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: \n",
            " [[ 8.151242  -2.8278337 -3.309761  -2.6220899]\n",
            " [ 8.174492  -2.9332068 -3.2482893 -2.5959346]\n",
            " [ 8.181815  -2.9110806 -3.2325187 -2.6634274]\n",
            " ...\n",
            " [-2.9325624 -3.0623033 -2.724237   7.6076756]\n",
            " [-3.140176  -3.2823186 -2.590203   7.8240466]\n",
            " [-3.1848192 -3.0800579  8.339697  -2.9060147]]\n",
            "\n",
            "Accuracy:  0.9632968448164843\n",
            "Precision:  [0.96992481 0.98123324 0.93160377 0.97478992]\n",
            "Recall:  [0.97974684 0.92424242 0.99246231 0.95604396]\n",
            "F1:  [0.97481108 0.95188557 0.96107056 0.96532594]\n",
            "['comp.windows.x', 'sci.med', 'soc.religion.christian', 'talk.politics.guns']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in ./my_model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.PREDICTIONS"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YdqRrsJufgPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "\n",
        "model = DistilBertForPostClassification.from_pretrained(\n",
        "    './my_model', config=distilbert.config, num_labels=len(categories)).to(device)\n",
        "for sentence in ['Lung cancer is a deadly disease.', 'God is love', 'How can you install Microsoft Office extensions?', 'Gun killings increase every year.']:\n",
        "  encoding = tokenizer.encode_plus(sentence)\n",
        "  encoding['input_ids'] = torch.tensor([encoding.input_ids]).to(device)\n",
        "  encoding['attention_mask'] = torch.tensor(encoding.attention_mask).to(device)\n",
        "  out = model(**encoding)\n",
        "  categories_probability = torch.nn.functional.softmax(out[0], dim=1).flatten()\n",
        "  print(sentence)\n",
        "  print('\\tProbabilities assigned by the model : ')\n",
        "  for n,c in enumerate(categories):\n",
        "    print('\\t\\t{} : {}'.format(c, categories_probability[n]))\n",
        "  print('\\n\\t--> Prediction :', categories[categories_probability.argmax()])\n",
        "  print('------------------------------------------------\\n')\n",
        "  "
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6fNKdHPnfgPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.IMPROVE THE MODEL"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BwRhEXFqfgPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# SOLUTION 1 (trivial): increase training epochs\n",
        "# SOLUTION 2: finetune encoder parameters too\n",
        "\n",
        "# model_unfreezed = DistilBertForPostClassification(config, freeze_decoder = False)\n",
        "# trainer_unfreezed = Trainer(\n",
        "#     model=model_unfreezed,                         \n",
        "#     args=training_args,                  \n",
        "#     train_dataset=train_dataset,         \n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "# trainer_unfreezed.train()\n",
        "# trainer_unfreezed.predict(test_dataset=test_dataset)\n",
        "\n",
        "# # SOLUTION 3: let's see what students can do !\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rH2Mxl0CfgPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REPORT\n",
        "\n",
        "## SOLUTION 0\n",
        "\n",
        "With the default given parameters:\n",
        "\n",
        "```py\n",
        "num_train_epochs=4,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "\n",
        "\n",
        "* Accuracy = 0.9001931745009659\n",
        "* Precision = [0.89277389 0.88235294 0.9 0.93030303]\n",
        "* Recall = [0.96962025 0.83333333 0.94974874 0.84340659]\n",
        "* F-score = [0.92961165 0.85714286 0.92420538 0.88472622]\n",
        "\n",
        "## SOLUTION 1\n",
        "\n",
        "We increase the number of training epochs to 10\n",
        "\n",
        "```py\n",
        "num_train_epochs=10,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "\n",
        "* Accuracy:  0.9336767546683837\n",
        "* Precision:  [0.93658537 0.96111111 0.92909535 0.90909091]\n",
        "* Recall:  [0.9721519  0.87373737 0.95477387 0.93406593]\n",
        "* F1:  [0.95403727 0.91534392 0.9417596  0.92140921]\n",
        "\n",
        "We can see that there has been a slight improvement in terms of accuracy as well as F-score when increasing the epochs to 10. Increasing beyond 10 *might* increase the scores little bit, but instead of doing that, let's take a look at some other solutions also.\n",
        "\n",
        "## SOLUTION 2\n",
        "\n",
        "This time, we keep the hyperparameters (specifically the no. of epochs) the same, but we unfreeze the encoder parameters.\n",
        "\n",
        "So in order to do so, we insert the folliwng snippet near the end of section \"4. MODEL\":\n",
        "\n",
        "```py\n",
        "model_unfreezed = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = False)\n",
        "```\n",
        "\n",
        "\n",
        "And this in section \"5. TRAINING\"\n",
        "\n",
        "```py\n",
        "trainer_unfreezed = Trainer(\n",
        "    model=model_unfreezed,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Freeze version\n",
        "# train_results = trainer.train()\n",
        "# test_results = trainer.predict(test_dataset=test_dataset)\n",
        "\n",
        "# Unfreeze version\n",
        "trainer_unfreezed.train()\n",
        "test_results = trainer_unfreezed.predict(test_dataset=test_dataset)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "As before, we kep our hyperparameters the same as previous solution\n",
        "```py\n",
        "num_train_epochs=10,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "And so, we get the following results: \n",
        "\n",
        "* Accuracy:  0.9639407598197038\n",
        "* Precision:  [0.96766169 0.98395722 0.92957746 0.98005698]\n",
        "* Recall:  [0.98481013 0.92929293 0.99497487 0.94505495]\n",
        "* F1:  [0.9761606  0.95584416 0.96116505 0.96223776]\n",
        "\n",
        "We see that unfreezing the parameters for encoder does improve the accuracy as well as F1 score.\n",
        "\n",
        "\n",
        "## SOLUTION 3\n",
        "\n",
        "As a first proposal, we can try to increase the number of layers in the model. We' re gonna try this with both freeze and unfreeze method.\n",
        "\n",
        "In the \"4. MODEL\" section, The following snippets were added:\n",
        "\n",
        "```py\n",
        "# SOLUTION 3\n",
        "# An extra proposed layer\n",
        "self.ExtraLayer = torch.nn.Linear(\n",
        "    in_features=config.dim, out_features=(config.dim-32), bias=True)\n",
        ".\n",
        ".\n",
        ".\n",
        "# SOLUTION 3\n",
        "# Feed into our extra layer\n",
        "pooled_output = self.ExtraLayer(pooled_output)\n",
        "# Pass it through the ReLU function\n",
        "pooled_output = F.relu(pooled_output)\n",
        "```\n",
        "\n",
        "| Epochs | Freeze | Linear Layers |          P          |          R          |          F          |  Acc |\n",
        "|:------:|:------:|:-------------:|:-------------------:|:-------------------:|:-------------------:|:----:|\n",
        "|   10   |   Yes  |    1 Extra    | 0.94 0.96 0.93 0.93 | 0.96 0.90 0.97 0.93 | 0.95 0.93 0.95 0.93 | 0.94 |\n",
        "|   10   |   No   |    1 Extra    | 0.96 0.98 0.93 0.97 | 0.97 0.92 0.99 0.95 | 0.97 0.95 0.96 0.96 | 0.96 |\n",
        "|        |        |               |                     |                     |                     |      |\n",
        "\n",
        "We observe that simply adding 1 linear layer improves the model a bit if we compare the 1st row with the SOLUTION 1 proposed earlier.\n",
        "But upon unfreezing the parameters, having this extra linear layer gives no additional benefit, it we comapre this 2nd row in the table with our previous SOLUTION 2.\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This BERT practical assignment gave us a hands-on experience of using the pre-trained transformer models provided by Hugging Face. Training the model with the best hyperparameters is tricky process but not an impossible one. Some bruth force techniques like Grid Search can be utilized in order to find the best hyperparameters.\n",
        "\n",
        "Possible extension of this assignment:\n",
        "\n",
        "* Use of a different model (like RoBERTa or GPT-2) and compare the results\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mgNJ7sdhRX7d"
      }
    }
  ]
}