{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e-cM3FPUE-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Managing arrays\n",
        "import numpy as np\n",
        "\n",
        "# load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available.\")\n",
        "  device = torch.cuda.current_device()\n",
        "else:\n",
        "  print(\"Will work on CPU.\")\n",
        "  \n",
        "  \n",
        "## DATA\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        " 'comp.windows.x',\n",
        " 'sci.med',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        "]\n",
        "\n",
        "# download data if not already present in data_home\n",
        "trainset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "testset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "\n",
        "# define input data and labels for training and testing\n",
        "x_train = trainset.data\n",
        "y_train = trainset.target\n",
        "x_test = testset.data\n",
        "y_test = testset.target\n",
        "\n",
        "# # SOLUTION (yes, we are cool)\n",
        "print('taille du jeu de donn√©es: \\n \\t {} posts de forum en total'.format(len(x_train) + len(x_test)))\n",
        "print('\\t {} posts pour le training'.format(len(x_train)))\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_train == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "  print('\\t {} posts pour le test'.format(len(x_test)))\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_test == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "\n",
        "print('\\n')\n",
        "print('EXEMPLE: \\n')\n",
        "print(x_train[0])\n",
        "\n",
        "\n",
        "def clean_post(post: str, remove_start: tuple):\n",
        "    clean_lines = []\n",
        "    for line in post.splitlines():\n",
        "            if not line.startswith(remove_start):\n",
        "                clean_lines.append(line)\n",
        "    return '\\n'.join(clean_lines)\n",
        "    \n",
        "\n",
        "# SOLUTION (yes, again, we are cool)\n",
        "remove_start = (\n",
        "  'From:',\n",
        "  'Subject:',\n",
        "  'Reply-To:',\n",
        "  'In-Reply-To:',\n",
        "  'Nntp-Posting-Host:',\n",
        "  'Organization:',\n",
        "  'X-Mailer:',\n",
        "  'In article <',\n",
        "  'Lines:',\n",
        "  'NNTP-Posting-Host:',\n",
        "  'Summary:',\n",
        "  'Article-I.D.:'\n",
        ")\n",
        "x_train = [clean_post(p, remove_start) for p in x_train]\n",
        "x_test = [clean_post(p, remove_start) for p in x_test]\n",
        "\n",
        "\n",
        "\n",
        "## TOKENISATION\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n",
        "\n",
        "# let's check out how the tokenizer works\n",
        "for n in range(3):\n",
        "    # tokenize forum post\n",
        "    tokenizer_out = tokenizer(x_train[n])\n",
        "    # convert numerical tokens to alphabetical tokens\n",
        "    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n",
        "    # decode tokens back to string\n",
        "    decoded = tokenizer.decode(tokenizer_out.input_ids)\n",
        "    print(tokenizer_out)\n",
        "    print(encoded_tok, '\\n')\n",
        "    print(decoded, '\\n')\n",
        "    print('---------------- \\n')\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "class PostsDataset(Dataset):\n",
        "    def __init__(self, posts, labels, tokenizer, max_len):\n",
        "        # variables that are set when the class is instantiated\n",
        "        self.posts = posts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.posts)\n",
        "  \n",
        "    def __getitem__(self, item):\n",
        "        # select the post and its category\n",
        "        post = str(self.posts[item])\n",
        "        label = self.labels[item]\n",
        "        # tokenize the post\n",
        "        tokenizer_out = self.tokenizer(\n",
        "            post,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "            )\n",
        "        # return a dictionary with the output of the tokenizer and the label\n",
        "        return  {\n",
        "            'input_ids': tokenizer_out['input_ids'].flatten(),\n",
        "            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# instantiate two PostsDatasets\n",
        "train_dataset = PostsDataset(x_train, y_train, tokenizer, MAX_LEN)\n",
        "test_dataset = PostsDataset(x_test, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "\n",
        "## MODEL\n",
        "\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        " \n",
        "first_post = train_dataset[0]\n",
        "\n",
        "hidden_state = distilbert(\n",
        "    input_ids=first_post['input_ids'].unsqueeze(0), attention_mask=first_post['attention_mask'].unsqueeze(0)\n",
        "    )\n",
        "\n",
        "print(hidden_state[0].shape)\n",
        "\n",
        "print(distilbert.config)\n",
        "\n",
        "\n",
        "from transformers import DistilBertPreTrainedModel, DistilBertConfig\n",
        "\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "class DistilBertForPostClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, freeze_encoder=False):\n",
        "        # instantiate the parent class DistilBertPreTrainedModel\n",
        "        super().__init__(config)\n",
        "        # instantiate num. of classes\n",
        "        self.num_labels = num_labels\n",
        "        # instantiate and load a pretrained DistilBERT model as encoder\n",
        "        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        # freeze the encoder parameters if required (Q1)\n",
        "        if freeze_encoder:\n",
        "          for param in self.encoder.parameters():\n",
        "              param.requires_grad = False\n",
        "        # the classifier: a feed-forward layer attached to the encoder's head\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            in_features=config.dim, out_features=self.num_labels, bias=True)\n",
        "        # instantiate a dropout function for the classifier's input\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "    ):\n",
        "        # encode a batch of sequences with DistilBERT\n",
        "        encoder_output = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        # extract the hidden representations from the encoder output\n",
        "        hidden_state = encoder_output[0]  # (bs, seq_len, dim)\n",
        "        # only select the encoding corresponding to the first token\n",
        "        # of each sequence in the batch (Q2)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        # apply dropout\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        # feed into the classifier\n",
        "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
        "\n",
        "        outputs = (logits,) + encoder_output[1:]\n",
        "        \n",
        "        if labels is not None: # (Q3)\n",
        "          # instantiate loss function\n",
        "          # SOLUTION : loss_fct = torch.nn.CrossEntropyLoss()\n",
        "          # calculate loss\n",
        "          # SOLUTION : loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "          # aggregate outputs\n",
        "          outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "# instantiate model\n",
        "model = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = True\n",
        "    )\n",
        "\n",
        "# print info about model's parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print('model total params: ', total_params)\n",
        "print('model trainable params: ', trainable_params)\n",
        "print('\\n', model)\n",
        "\n",
        "\n",
        "## TRAINING\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    logging_dir='./logs',\n",
        "    logging_first_step=True,\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=4,              \n",
        "    per_device_train_batch_size=8,  \n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01        \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "train_results = trainer.train()\n",
        "\n",
        "test_results = trainer.predict(test_dataset=test_dataset)\n",
        "\n",
        "print('Predictions: \\n', test_results.predictions)\n",
        "print('\\nAccuracy: ', test_results.metrics['eval_accuracy'])\n",
        "print('Precision: ', test_results.metrics['eval_precision'])\n",
        "print('Recall: ', test_results.metrics['eval_recall'])\n",
        "print('F1: ', test_results.metrics['eval_f1'])\n",
        "print(categories)\n",
        "\n",
        "MODEL_PATH = './my_model'\n",
        "trainer.save_model(MODEL_PATH)\n",
        "\n",
        "\n",
        "## PREDICTIONS\n",
        "\n",
        "model = DistilBertForPostClassification.from_pretrained(\n",
        "    './my_model', config=distilbert.config, num_labels=len(categories)).to(device)\n",
        "for sentence in ['Lung cancer is a deadly disease.', 'God is love', 'How can you install Microsoft Office extensions?', 'Gun killings increase every year.']:\n",
        "  encoding = tokenizer.encode_plus(sentence)\n",
        "  encoding['input_ids'] = torch.tensor([encoding.input_ids]).to(device)\n",
        "  encoding['attention_mask'] = torch.tensor(encoding.attention_mask).to(device)\n",
        "  out = model(**encoding)\n",
        "  categories_probability = torch.nn.functional.softmax(out[0], dim=1).flatten()\n",
        "  print(sentence)\n",
        "  print('\\tProbabilities assigned by the model : ')\n",
        "  for n,c in enumerate(categories):\n",
        "    print('\\t\\t{} : {}'.format(c, categories_probability[n]))\n",
        "  print('\\n\\t--> Prediction :', categories[categories_probability.argmax()])\n",
        "  print('------------------------------------------------\\n')\n",
        "  \n",
        "  \n",
        "  \n",
        "## IMPROVE THE MODEL\n",
        "\n",
        "# # SOLUTION 1 (trivial): increase training epochs\n",
        "# # SOLUTION 2: finetune encoder parameters too\n",
        "\n",
        "# model_unfreezed = DistilBertForPostClassification(config, freeze_decoder = False)\n",
        "# trainer_unfreezed = Trainer(\n",
        "#     model=model_unfreezed,                         \n",
        "#     args=training_args,                  \n",
        "#     train_dataset=train_dataset,         \n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "# trainer_unfreezed.train()\n",
        "# trainer_unfreezed.predict(test_dataset=test_dataset)\n",
        "\n",
        "# # SOLUTION 3: let's see what students can do !\n"
      ]
    }
  ]
}