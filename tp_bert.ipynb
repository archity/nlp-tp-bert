{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fPaN19rhfgPN",
        "L2mmpMT6fgPO",
        "eY2w_htDfgPQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b53b27c594d41298356de3dfd4d3db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8af91b8f743b482fa32c5e9ec606b36e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8a6ae2488e7040c897a4c69bff11c426",
              "IPY_MODEL_ab3b3b1372d64cfba6eaca6de1567055",
              "IPY_MODEL_49d7168cd6384bb0a0902a9234c54039"
            ]
          }
        },
        "8af91b8f743b482fa32c5e9ec606b36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a6ae2488e7040c897a4c69bff11c426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c48bacb555e46d3b2e235def875e23b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fe5051141a64d5690e0e26c97184698"
          }
        },
        "ab3b3b1372d64cfba6eaca6de1567055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82d3516922da41ac843161e6f8a28eee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ecc50123b704ce788e887937e1e9c7a"
          }
        },
        "49d7168cd6384bb0a0902a9234c54039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52ebf3c54f1444fab71549cf2102fae9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 256M/256M [00:09&lt;00:00, 31.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51981c4cf6fb4f7eb7a25a2cbfd48f1c"
          }
        },
        "8c48bacb555e46d3b2e235def875e23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fe5051141a64d5690e0e26c97184698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82d3516922da41ac843161e6f8a28eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ecc50123b704ce788e887937e1e9c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52ebf3c54f1444fab71549cf2102fae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51981c4cf6fb4f7eb7a25a2cbfd48f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Practical\n",
        "\n",
        "Practical work on BERT for the course Natural Language Processing in M2 MoSIG\n",
        "\n",
        "> Author: Archit YADAV"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Iaxs5T3cfgPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. IMPORT MODULES"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CCsTMRfzfgPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "7hdlQuifgDMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4e-cM3FPUE-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Managing arrays\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "# load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available.\")\n",
        "  device = torch.cuda.current_device()\n",
        "else:\n",
        "  print(\"Will work on CPU.\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk0dCwOmfgPM",
        "outputId": "420062b7-6ede-46d5-885d-b0516c2edf9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. DATA"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fPaN19rhfgPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Downloading of Data"
      ],
      "metadata": {
        "id": "_sr62iV-hYRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: \n",
            "3885 posts in total\n",
            "\t 2332 training posts\n",
            "\t\t 593 comp.windows.x\n",
            "\t\t 594 sci.med\n",
            "\t\t 599 soc.religion.christian\n",
            "\t\t 546 talk.politics.guns\n",
            "\t 1553 testing posts\n",
            "\t\t 395 comp.windows.x\n",
            "\t\t 396 sci.med\n",
            "\t\t 398 soc.religion.christian\n",
            "\t\t 364 talk.politics.guns\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        " 'comp.windows.x',\n",
        " 'sci.med',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        "]\n",
        "\n",
        "# Download data if not already present in data_home\n",
        "trainset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "testset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
        "\n",
        "# Define input data and labels for training and testing\n",
        "x_train = trainset.data\n",
        "y_train = trainset.target\n",
        "x_test = testset.data\n",
        "y_test = testset.target\n",
        "\n",
        "# SOLUTION (yes, we are cool)\n",
        "print('Dataset size: \\n{} posts in total'.format(len(x_train) + len(x_test)))\n",
        "print('\\t {} training posts'.format(len(x_train)))\n",
        "\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_train == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "\n",
        "print('\\t {} testing posts'.format(len(x_test)))\n",
        "for i in range(len(categories)):\n",
        "  num = sum(y_test == i)\n",
        "  print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
        "\n",
        "\n",
        "\n",
        "# print('\\n')\n",
        "# print('EXAMPLE: \\n')\n",
        "# print(x_train[0])\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaiRgy0yfgPN",
        "outputId": "8901e6f1-45fb-4453-92e9-e00ff7a8231e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Cleaning of Data"
      ],
      "metadata": {
        "id": "MRgRZaothcE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove lines starting with certain keywords \n",
        "def clean_post(post: str, remove_start: tuple):\n",
        "    clean_lines = []\n",
        "    for line in post.splitlines():\n",
        "            if not line.startswith(remove_start):\n",
        "                clean_lines.append(line)\n",
        "    return '\\n'.join(clean_lines)\n",
        "    \n",
        "\n",
        "# SOLUTION (yes, again, we are cool)\n",
        "remove_start = (\n",
        "  'From:',\n",
        "  'Subject:',\n",
        "  'Reply-To:',\n",
        "  'In-Reply-To:',\n",
        "  'Nntp-Posting-Host:',\n",
        "  'Organization:',\n",
        "  'X-Mailer:',\n",
        "  'In article <',\n",
        "  'Lines:',\n",
        "  'NNTP-Posting-Host:',\n",
        "  'Summary:',\n",
        "  'Article-I.D.:'\n",
        ")\n",
        "x_train = [clean_post(p, remove_start) for p in x_train]\n",
        "x_test = [clean_post(p, remove_start) for p in x_test]\n"
      ],
      "metadata": {
        "id": "Ma4tJ6NKhTSR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TOKENISATION"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "L2mmpMT6fgPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n",
        "\n",
        "# Let's check out how the tokenizer works\n",
        "for n in range(3):\n",
        "    # Tokenize forum post\n",
        "    tokenizer_out = tokenizer(x_train[n])\n",
        "    # Convert numerical tokens to alphabetical tokens\n",
        "    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n",
        "    # Decode tokens back to string\n",
        "    decoded = tokenizer.decode(tokenizer_out.input_ids)\n",
        "    print(tokenizer_out)\n",
        "    print(encoded_tok, '\\n')\n",
        "    print(decoded, '\\n')\n",
        "    print('---------------- \\n')\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QaJLo-4wfgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "class PostsDataset(Dataset):\n",
        "    def __init__(self, posts, labels, tokenizer, max_len):\n",
        "        # Variables that are set when the class is instantiated\n",
        "        self.posts = posts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.posts)\n",
        "  \n",
        "    def __getitem__(self, item):\n",
        "        # Select the post and its category\n",
        "        post = str(self.posts[item])\n",
        "        label = self.labels[item]\n",
        "        # Tokenize the post\n",
        "        tokenizer_out = self.tokenizer(\n",
        "            post,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "            )\n",
        "        # Return a dictionary with the output of the tokenizer and the label\n",
        "        return  {\n",
        "            'input_ids': tokenizer_out['input_ids'].flatten(),\n",
        "            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Instantiate two PostsDatasets\n",
        "train_dataset = PostsDataset(x_train, y_train, tokenizer, MAX_LEN)\n",
        "test_dataset = PostsDataset(x_test, y_test, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "Q-TwXpvoksBc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. MODEL"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eY2w_htDfgPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b53b27c594d41298356de3dfd4d3db7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512, 768])\n",
            "DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.16.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        " \n",
        "first_post = train_dataset[0]\n",
        "\n",
        "hidden_state = distilbert(\n",
        "    input_ids=first_post['input_ids'].unsqueeze(0),\n",
        "    attention_mask=first_post['attention_mask'].unsqueeze(0)\n",
        "    )\n",
        "\n",
        "print(hidden_state[0].shape)\n",
        "print(distilbert.config)\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xnhWAdLNfgPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590,
          "referenced_widgets": [
            "3b53b27c594d41298356de3dfd4d3db7",
            "8af91b8f743b482fa32c5e9ec606b36e",
            "8a6ae2488e7040c897a4c69bff11c426",
            "ab3b3b1372d64cfba6eaca6de1567055",
            "49d7168cd6384bb0a0902a9234c54039",
            "8c48bacb555e46d3b2e235def875e23b",
            "8fe5051141a64d5690e0e26c97184698",
            "82d3516922da41ac843161e6f8a28eee",
            "0ecc50123b704ce788e887937e1e9c7a",
            "52ebf3c54f1444fab71549cf2102fae9",
            "51981c4cf6fb4f7eb7a25a2cbfd48f1c"
          ]
        },
        "outputId": "00bb91e3-bbdf-4f27-c88b-7b5d7ff8fc82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertPreTrainedModel, DistilBertConfig\n",
        "\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "class DistilBertForPostClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, freeze_encoder=False):\n",
        "        # Instantiate the parent class DistilBertPreTrainedModel\n",
        "        super().__init__(config)\n",
        "        # Instantiate num. of classes\n",
        "        self.num_labels = num_labels\n",
        "        # Instantiate and load a pretrained DistilBERT model as encoder\n",
        "        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        # Freeze the encoder parameters if required (Q1)\n",
        "        if freeze_encoder:\n",
        "          for param in self.encoder.parameters():\n",
        "              param.requires_grad = False\n",
        "        # The classifier: a feed-forward layer attached to the encoder's head\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            in_features=config.dim, out_features=self.num_labels, bias=True)\n",
        "        # Instantiate a dropout function for the classifier's input\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "    ):\n",
        "        # Encode a batch of sequences with DistilBERT\n",
        "        encoder_output = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        # Extract the hidden representations from the encoder output\n",
        "        hidden_state = encoder_output[0]  # (bs, seq_len, dim)\n",
        "        # Only select the encoding corresponding to the first token\n",
        "        # of each sequence in the batch (Q2)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        # Apply dropout\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        # Feed into the classifier\n",
        "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
        "\n",
        "        outputs = (logits,) + encoder_output[1:]\n",
        "        \n",
        "        if labels is not None: # (Q3)\n",
        "          # Instantiate loss function\n",
        "          # SOLUTION :\n",
        "          loss_fct = torch.nn.CrossEntropyLoss()\n",
        "          # Calculate loss\n",
        "          # SOLUTION :\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "          # Aggregate outputs\n",
        "          outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "model = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = True\n",
        "    )\n",
        "\n",
        "model_unfreezed = DistilBertForPostClassification(\n",
        "    config=distilbert.config, num_labels=len(categories), freeze_encoder = False)\n",
        "\n",
        "# Print info about model's parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print('Model total params: ', total_params)\n",
        "print('Model trainable params: ', trainable_params)\n",
        "print('\\n', model)"
      ],
      "metadata": {
        "id": "pCuSAhqRnFvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TRAINING"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b1sqdw_-fgPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    logging_dir='./logs',\n",
        "    logging_first_step=True,\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=10,              \n",
        "    per_device_train_batch_size=8,  \n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01        \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_unfreezed = Trainer(\n",
        "    model=model_unfreezed,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Freeze version\n",
        "# train_results = trainer.train()\n",
        "# test_results = trainer.predict(test_dataset=test_dataset)\n",
        "\n",
        "# Unfreeze version\n",
        "trainer_unfreezed.train()\n",
        "test_results = trainer_unfreezed.predict(test_dataset=test_dataset)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yzNYv5-nfgPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Predictions: \\n', test_results.predictions)\n",
        "print('\\nAccuracy: ', test_results.metrics['test_accuracy'])\n",
        "print('Precision: ', test_results.metrics['test_precision'])\n",
        "print('Recall: ', test_results.metrics['test_recall'])\n",
        "print('F1: ', test_results.metrics['test_f1'])\n",
        "print(categories)\n",
        "\n",
        "MODEL_PATH = './my_model'\n",
        "trainer.save_model(MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB5679XnXkHH",
        "outputId": "6c583550-9472-4cae-d209-b86052521a36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./my_model\n",
            "Configuration saved in ./my_model/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: \n",
            " [[ 8.573914  -2.9970162 -3.0946548 -3.062675 ]\n",
            " [ 8.604912  -2.948413  -3.0928211 -3.0152574]\n",
            " [ 8.539274  -2.9237611 -3.182105  -3.0524912]\n",
            " ...\n",
            " [-2.379094  -3.3811877 -2.9291563  7.4395003]\n",
            " [-2.4381752 -3.3345978 -3.2704394  7.7305355]\n",
            " [-3.092416  -2.7200103  8.573861  -2.7820218]]\n",
            "\n",
            "Accuracy:  0.9639407598197038\n",
            "Precision:  [0.96766169 0.98395722 0.92957746 0.98005698]\n",
            "Recall:  [0.98481013 0.92929293 0.99497487 0.94505495]\n",
            "F1:  [0.9761606  0.95584416 0.96116505 0.96223776]\n",
            "['comp.windows.x', 'sci.med', 'soc.religion.christian', 'talk.politics.guns']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in ./my_model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. PREDICTIONS"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YdqRrsJufgPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "\n",
        "model = DistilBertForPostClassification.from_pretrained(\n",
        "    './my_model', config=distilbert.config, num_labels=len(categories)).to(device)\n",
        "for sentence in ['Lung cancer is a deadly disease.', 'God is love', 'How can you install Microsoft Office extensions?', 'Gun killings increase every year.']:\n",
        "  encoding = tokenizer.encode_plus(sentence)\n",
        "  encoding['input_ids'] = torch.tensor([encoding.input_ids]).to(device)\n",
        "  encoding['attention_mask'] = torch.tensor(encoding.attention_mask).to(device)\n",
        "  out = model(**encoding)\n",
        "  categories_probability = torch.nn.functional.softmax(out[0], dim=1).flatten()\n",
        "  print(sentence)\n",
        "  print('\\tProbabilities assigned by the model : ')\n",
        "  for n,c in enumerate(categories):\n",
        "    print('\\t\\t{} : {}'.format(c, categories_probability[n]))\n",
        "  print('\\n\\t--> Prediction :', categories[categories_probability.argmax()])\n",
        "  print('------------------------------------------------\\n')\n",
        "  "
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6fNKdHPnfgPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. IMPROVE THE MODEL"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BwRhEXFqfgPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# SOLUTION 1 (trivial): increase training epochs\n",
        "# SOLUTION 2: finetune encoder parameters too\n",
        "\n",
        "# model_unfreezed = DistilBertForPostClassification(config, freeze_decoder = False)\n",
        "# trainer_unfreezed = Trainer(\n",
        "#     model=model_unfreezed,                         \n",
        "#     args=training_args,                  \n",
        "#     train_dataset=train_dataset,         \n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "# trainer_unfreezed.train()\n",
        "# trainer_unfreezed.predict(test_dataset=test_dataset)\n",
        "\n",
        "# # SOLUTION 3: let's see what students can do !\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rH2Mxl0CfgPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remarks - Report\n",
        "\n",
        "## Solution 0\n",
        "\n",
        "With the default given parameters:\n",
        "\n",
        "```py\n",
        "num_train_epochs=4,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "\n",
        "\n",
        "* Accuracy = 0.9001931745009659\n",
        "* Precision = [0.89277389 0.88235294 0.9 0.93030303]\n",
        "* Recall = [0.96962025 0.83333333 0.94974874 0.84340659]\n",
        "* F-score = [0.92961165 0.85714286 0.92420538 0.88472622]\n",
        "\n",
        "## Solution 1\n",
        "\n",
        "We increase the number of training epochs to 10\n",
        "\n",
        "```py\n",
        "num_train_epochs=10,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "\n",
        "* Accuracy:  0.9336767546683837\n",
        "* Precision:  [0.93658537 0.96111111 0.92909535 0.90909091]\n",
        "* Recall:  [0.9721519  0.87373737 0.95477387 0.93406593]\n",
        "* F1:  [0.95403727 0.91534392 0.9417596  0.92140921]\n",
        "\n",
        "We can see that there has been a slight improvement in terms of accuracy as well as F-score when increasing the epochs to 10. Increasing beyond 10 *might* increase the scores little bit, but instead of doing that, let's take a look at some other solutions also.\n",
        "\n",
        "## Solution 2\n",
        "\n",
        "This time, we keep the hyperparameters (specifically the no. of epochs) the same, but we unfreeze the encoder parameters.\n",
        "\n",
        "So in order to do so, we insert the folliwng snippet near the end of section \"5. TRAINING\":\n",
        "\n",
        "```py\n",
        "trainer_unfreezed = Trainer(\n",
        "    model=model_unfreezed,                         \n",
        "    args=training_args,                  \n",
        "    train_dataset=train_dataset,         \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Freeze version\n",
        "# train_results = trainer.train()\n",
        "# test_results = trainer.predict(test_dataset=test_dataset)\n",
        "\n",
        "# Unfreeze version\n",
        "trainer_unfreezed.train()\n",
        "test_results = trainer_unfreezed.predict(test_dataset=test_dataset)\n",
        "```\n",
        "\n",
        "As before, we kep our hyperparameters the same as previous solution\n",
        "```py\n",
        "num_train_epochs=10,\n",
        "per_device_train_batch_size=8,\n",
        "learning_rate=5e-5,\n",
        "weight_decay=0.01\n",
        "```\n",
        "\n",
        "* Accuracy:  0.9639407598197038\n",
        "* Precision:  [0.96766169 0.98395722 0.92957746 0.98005698]\n",
        "* Recall:  [0.98481013 0.92929293 0.99497487 0.94505495]\n",
        "* F1:  [0.9761606  0.95584416 0.96116505 0.96223776]\n",
        "\n",
        "We see that unfreezing the parameters for encoder does improve the accuracy as well as F1 score.\n",
        "\n",
        "\n",
        "## Solution 3\n",
        "\n"
      ],
      "metadata": {
        "id": "mgNJ7sdhRX7d"
      }
    }
  ]
}